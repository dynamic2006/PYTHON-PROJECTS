{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea6f87c-48b7-433d-b9e7-6df06de6959f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5b98e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 17:53:26.703630: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-08 17:53:26.741878: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-08 17:53:26.742685: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-08 17:53:27.355889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ee297-0faa-4e93-9ff7-a3d8b021a651",
   "metadata": {},
   "source": [
    "2. Draw Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ea0e05-2585-4371-8415-cb6cdf7129b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b08bc69-d864-420b-8641-3968e9a14e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert bgr -> rgb\n",
    "    image.flags.writeable = False # set to non-changeable\n",
    "    results = model.process(image) # make prediction with model\n",
    "    image.flags.writeable = True # set to changeable\n",
    "    iamge = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # convert rgb -> bgr\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f7dfef-cd5d-48f5-9998-81ed68531818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) # draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1539f66c-9d00-49b0-b0e3-48e72195e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    \n",
    "    # draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                              mp_drawing.DrawingSpec(color = (80, 110, 10) , thickness = 1, circle_radius = 1), \n",
    "                              mp_drawing.DrawingSpec(color = (80, 256, 121), thickness = 1, circle_radius = 1))\n",
    "\n",
    "    # draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color = (80, 22, 10) , thickness = 2, circle_radius = 4), \n",
    "                              mp_drawing.DrawingSpec(color = (80, 44, 121), thickness = 2, circle_radius = 2))\n",
    "\n",
    "    # draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color = (121, 22, 76) , thickness = 2, circle_radius = 4), \n",
    "                              mp_drawing.DrawingSpec(color = (121, 44, 250), thickness = 2, circle_radius = 2))\n",
    "\n",
    "    # draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color = (245, 117, 66) , thickness = 2, circle_radius = 4), \n",
    "                              mp_drawing.DrawingSpec(color = (245, 66, 230), thickness = 2, circle_radius = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6505fc21-4404-4043-9f62-e1b260eac463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(2)\n",
    "\n",
    "# set mediapipe model\n",
    "# intial detection, then mediapipe tracks landmarks\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "    \n",
    "        # read feed\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        # make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        # draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "    \n",
    "        # show to screen\n",
    "        cv2.imshow(\"OpenCV Feed\" , cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "        # break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6d573-42a3-4ec5-9a23-84b96410c356",
   "metadata": {},
   "source": [
    "3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa617cd5-a7f7-445c-bace-965c59abc1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "\n",
    "    pose = np.zeros(33*4)\n",
    "    if results.pose_landmarks:\n",
    "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten()\n",
    "\n",
    "    face = np.zeros(468*3)\n",
    "    if results.face_landmarks:\n",
    "        face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten()\n",
    "\n",
    "    lh = np.zeros(21*3)\n",
    "    if results.left_hand_landmarks:\n",
    "        lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten()\n",
    "\n",
    "    rh = np.zeros(21*3)\n",
    "    if results.right_hand_landmarks:\n",
    "        rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten()\n",
    "\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c35ff7-4a90-48bf-8ab3-4fd860071a7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_test \u001b[38;5;241m=\u001b[39m extract_keypoints(\u001b[43mresults\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99b429b1-97ac-41c5-92d2-3d180557de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"0\", result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d1a7a2b-e5e5-41f4-99ff-9007c63d5d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44014844,  0.42120978, -1.40003324, ...,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"0.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deb5515-9ff3-4550-b67c-dc1dbea5cacb",
   "metadata": {},
   "source": [
    "4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41f0aa39-1333-4b54-9ad5-a8bd712d4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(\"MP_Data\") # path for exported data\n",
    "actions = np.array([\"hello\", \"thanks\", \"iloveyou\"]) # categories\n",
    "no_sequences = 30 # videos\n",
    "sequence_length = 30 # frames per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "076ebe8e-3184-40bc-9f25-f5684fff7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops through 3 actions\n",
    "for action in actions:\n",
    "    # loops through 30 sequences (30 videos)\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            # make a folder for each video -- numbered 0 to 29\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc224c0-c259-49ba-90a4-e0c1d3e26ad8",
   "metadata": {},
   "source": [
    "5. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7174b2a-37aa-41d8-b13f-9948a70aef1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(2)\n",
    "\n",
    "# set mediapipe model\n",
    "# intial detection, then mediapipe tracks landmarks\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "\n",
    "    # NEW loop through actions:\n",
    "    for action in actions:\n",
    "        # loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # loop through frames in video\n",
    "            for frame_num in range(sequence_length):\n",
    "                \n",
    "                # read feed\n",
    "                ret, frame = cap.read()\n",
    "            \n",
    "                # make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "                # draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "\n",
    "                # NEW apply wait logic\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, \"STARTING COLLECTION\", (120, 200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(image, \"Collecting frames for {} Video Number {}\".format(action, sequence), (15, 12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image, \"Collecting frames for {} Video Number {}\".format(action, sequence), (15, 12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                # NEW export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "            \n",
    "                # show to screen\n",
    "                cv2.imshow(\"OpenCV Feed\" , cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "                # break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc42b526-fb2f-4364-b188-86f123164618",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8075a5-426b-4562-ae25-9ab4881dd32d",
   "metadata": {},
   "source": [
    "6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c653529-8098-42e7-87f5-3822673abaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15d2f3dc-c92b-42b3-bac2-6bcd67611ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label : num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e3edd25-de80-482a-8769-4fd7f2748a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'thanks': 1, 'iloveyou': 2}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cde0337-2e94-4a59-b31e-4203f135f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ac5a69b-e78a-4605-88e3-c76e87a19225",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c5ce744-0a42-4cc2-8a9a-eef89630a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab76f93-b3f7-457d-9e25-dbc9e40b56cc",
   "metadata": {},
   "source": [
    "7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42b93aa6-d704-452a-97fe-5849a5f45718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ac463dc-f536-40fc-a60f-6e539ff1001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\"Logs\")\n",
    "tb_callback = TensorBoard(log_dir = log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "445ded93-6edb-4518-946e-c3d3dd6af1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(64, return_sequences = True, activation = \"relu\", input_shape = (30, 1662)))\n",
    "model.add(LSTM(128, return_sequences = True, activation = \"relu\"))\n",
    "model.add(LSTM(64, return_sequences = False, activation = \"relu\"))\n",
    "\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dense(actions.shape[0], activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6c0738c-6799-4262-8bbb-7741ad1b391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"Adam\", loss = \"categorical_crossentropy\" , metrics = [\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fffa4f-4316-4c07-b311-bd72e214f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs = 500, callbacks = [tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49379f0b-0d2e-4c52-a22d-744fada26519",
   "metadata": {},
   "source": [
    "8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0d61dad-541c-4268-87ef-47245ba043ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 298ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2579791a-97b8-4439-944c-2d9ec0be265a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "015fa6ce-ffed-46a6-a187-744e48a3a6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acdda1c-ca68-4ea5-b055-0f1744d4b990",
   "metadata": {},
   "source": [
    "9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b3078ea-6b60-44ac-a67b-d48bbb3b8ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tejasvi/Desktop/CODING/PYTHON/kaggle/env/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"action.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a47b6f24-3712-420d-b093-322a4bfc25d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c77b7ac3-49d3-49c8-b3f6-d95c771f4c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"action.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378ad8f7-8917-4859-9769-173a3f8a285d",
   "metadata": {},
   "source": [
    "10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "583ed46e-a978-47d2-ba7e-390f83cbad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e78f00a7-6524-45b2-9340-275bc22228c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ce94844-9870-4091-b992-ffb3c2e2518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_train, axis = 1).tolist()\n",
    "yhat = np.argmax(yhat, axis = 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f71ec290-1056-45a6-9eb0-8e4054c833f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[58,  0],\n",
       "        [ 0, 27]],\n",
       "\n",
       "       [[56,  0],\n",
       "        [ 0, 29]],\n",
       "\n",
       "       [[56,  0],\n",
       "        [ 0, 29]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d57c3297-5501-49e9-97cc-c2189b06a952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22977b5d-cfe9-4ba8-a772-b5532aa7bbae",
   "metadata": {},
   "source": [
    "11. Test in Real Time!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
